#!/usr/bin/env python3
"""
ä½ä»·+ç¨³å®šå¢é•¿é€‰è‚¡å™¨ - å†…å­˜å‹å¥½ç‰ˆæœ¬
=============================================

åŸºäºLowPriceGrowthSelectorçš„å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒï¼š
- æ»šåŠ¨çª—å£è®­ç»ƒï¼ˆæ¯æ¬¡åªåŠ è½½2-3å¹´æ•°æ®ï¼‰
- Parquetåˆ†åŒºå­˜å‚¨åŸºæœ¬é¢æ•°æ®
- å†…å­˜é™åˆ¶å’Œç›‘æ§
- æ”¯æŒå®Œæ•´11å¹´æ•°æ®è®­ç»ƒï¼Œå†…å­˜æ§åˆ¶åœ¨4-8GB

Usage:
    # æ»šåŠ¨è®­ç»ƒï¼ˆæ¨èï¼‰
    python lowprice_growth_selector_light.py --rolling_train --window_years 3 --max_ram_gb 4
    
    # æ¨ç†æœ€æ–°çª—å£
    python lowprice_growth_selector_light.py --predict_latest
    
    # å®Œæ•´å›æµ‹
    python lowprice_growth_selector_light.py --backtest_stocks --topk 10
"""

import os
import sys
import argparse
import warnings
import logging
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime, timedelta
import json
from tqdm import tqdm
import gc
import glob

# æ·»åŠ çˆ¶ç±»è·¯å¾„
sys.path.append(str(Path(__file__).parent))
from lowprice_growth_selector import LowPriceGrowthSelector, CONFIG

# æ–°å¢ä¾èµ–
try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    print("âœ“ PyArrowå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print("âŒ PyArrowå¯¼å…¥å¤±è´¥ï¼Œè¯·å®‰è£…: pip install pyarrow>=15.0")
    sys.exit(1)

warnings.filterwarnings('ignore')

# å†…å­˜å‹å¥½é…ç½®
LIGHT_CONFIG = {
    **CONFIG,
    'chunk_size_years': 2,            # åˆ†å—åŠ è½½å¹´æ•°
    'enable_chunked_loading': True,   # å¯ç”¨åˆ†å—åŠ è½½
    'parquet_partition_cols': ['code'],  # Parquetåˆ†åŒºåˆ—
    'max_ram_gb': 4,                  # é»˜è®¤æœ€å¤§å†…å­˜é™åˆ¶
    'dtype_optimization': 'float32',  # æ•°æ®ç±»å‹ä¼˜åŒ–
}

logger = logging.getLogger(__name__)


class LowPriceGrowthSelectorLight(LowPriceGrowthSelector):
    """å†…å­˜å‹å¥½çš„ä½ä»·æˆé•¿é€‰è‚¡å™¨"""
    
    def __init__(self, 
                 start_date: str = LIGHT_CONFIG['start_date'],
                 end_date: str = LIGHT_CONFIG['end_date'],
                 universe: str = LIGHT_CONFIG['universe'],
                 max_ram_gb: int = LIGHT_CONFIG['max_ram_gb']):
        """
        åˆå§‹åŒ–è½»é‡çº§é€‰è‚¡å™¨
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            universe: è‚¡ç¥¨æ± 
            max_ram_gb: æœ€å¤§å†…å­˜é™åˆ¶(GB)
        """
        # æ›´æ–°å†…å­˜é…ç½®
        CONFIG['max_memory_gb'] = max_ram_gb
        LIGHT_CONFIG['max_ram_gb'] = max_ram_gb
        
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__(start_date, end_date, universe)
        
        # å†…å­˜å‹å¥½è®¾ç½®
        self.max_ram_gb = max_ram_gb
        self.fundamental_parquet_dir = self.output_dir / 'fundamental_parquet'
        self.fundamental_parquet_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸš€ å†…å­˜å‹å¥½æ¨¡å¼å·²å¯ç”¨ï¼Œæœ€å¤§å†…å­˜é™åˆ¶: {max_ram_gb}GB")
    
    def fetch_fundamental_data_parquet(self, force_refresh: bool = False) -> pd.DataFrame:
        """
        è·å–åŸºæœ¬é¢æ•°æ®å¹¶ä¿å­˜ä¸ºParquetåˆ†åŒºæ ¼å¼
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ•°æ®
            
        Returns:
            åŸºæœ¬é¢æ•°æ®DataFrame
        """
        logger.info("ğŸ—ƒï¸ å¼€å§‹è·å–åŸºæœ¬é¢æ•°æ®ï¼ˆParquetæ ¼å¼ï¼‰...")
        
        # æ£€æŸ¥Parquetç¼“å­˜
        if not force_refresh and len(list(self.fundamental_parquet_dir.glob('*'))) > 0:
            logger.info("åŠ è½½ç¼“å­˜çš„ParquetåŸºæœ¬é¢æ•°æ®...")
            return self._load_fundamental_parquet()
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_codes = self._get_stock_list()
        
        # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨å¹¶ä¿å­˜ä¸ºParquet
        failed_stocks = []
        
        for i, code in enumerate(tqdm(stock_codes, desc="è·å–åŸºæœ¬é¢æ•°æ®")):
            try:
                self._fetch_and_save_stock_fundamental(code)
                
                # å†…å­˜ç®¡ç†ï¼šæ¯100åªè‚¡ç¥¨å¼ºåˆ¶åƒåœ¾å›æ”¶
                if i % 100 == 0 and i > 0:
                    self._log_memory_usage(f"å·²å¤„ç†{i}åªè‚¡ç¥¨")
                    self._force_gc()
                    
            except Exception as e:
                logger.warning(f"è‚¡ç¥¨ {code} åŸºæœ¬é¢æ•°æ®è·å–å¤±è´¥: {e}")
                failed_stocks.append(code)
        
        logger.info(f"åŸºæœ¬é¢æ•°æ®è·å–å®Œæˆï¼Œå¤±è´¥è‚¡ç¥¨æ•°: {len(failed_stocks)}")
        
        # è¿”å›å®Œæ•´æ•°æ®
        return self._load_fundamental_parquet()
    
    def _get_stock_list(self) -> List[str]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        if self.universe == 'all':
            # æ‰©å±•é¢„å®šä¹‰è‚¡ç¥¨æ± 
            stock_codes = [
                'SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002',
                'SZ300015', 'SZ002415', 'SH600276', 'SH600030', 'SZ000858',
                'SH600585', 'SH601318', 'SZ002594', 'SH600900', 'SZ002230',
                'SH601398', 'SH601939', 'SZ000002', 'SH600887', 'SZ002304'
            ]
            logger.info(f"ä½¿ç”¨æ‰©å±•é¢„å®šä¹‰è‚¡ç¥¨æ± : {len(stock_codes)}åªè‚¡ç¥¨")
            return stock_codes
        else:
            # ä½¿ç”¨çˆ¶ç±»æ–¹æ³•
            try:
                from qlib.data import D
                instruments = D.list_instruments(
                    instruments=self.universe, 
                    start_time=self.start_date, 
                    end_time=self.end_date
                )
                if isinstance(instruments, (list, tuple)):
                    return [inst for inst in instruments if isinstance(inst, str)]
                else:
                    return ['SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002']
            except:
                return ['SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002']
    
    def _fetch_and_save_stock_fundamental(self, code: str):
        """è·å–å•ä¸ªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®å¹¶ä¿å­˜ä¸ºParquet"""
        try:
            import akshare as ak
            
            # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼
            ak_code = code.replace('SH', '').replace('SZ', '')
            
            # è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ®
            financial_data = ak.stock_financial_analysis_indicator_em(symbol=ak_code)
            if financial_data.empty:
                return
            
            # æ•°æ®æ¸…ç†
            financial_data['date'] = pd.to_datetime(financial_data['date'])
            financial_data['code'] = code
            
            # æ‰“å°åˆ—åä»¥ä¾¿è°ƒè¯•
            logger.info(f"è·å–åˆ°çš„åˆ—å: {list(financial_data.columns)}")
            
            # æ›´çµæ´»çš„å­—æ®µåŒ¹é…
            # å°è¯•åŒ¹é…å¸¸è§çš„åˆ—åå˜ä½“
            column_mappings = {
                # åŸå§‹åç§° -> æ ‡å‡†åç§°
                'å‡€åˆ©æ¶¦': 'net_profit',
                'å‡€åˆ©æ¶¦-å•å­£åº¦': 'net_profit',
                'å‡€åˆ©æ¶¦(ä¸‡å…ƒ)': 'net_profit',
                'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿': 'net_profit_yoy',
                'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡': 'net_profit_yoy',
                'è¥ä¸šæ€»æ”¶å…¥': 'revenue', 
                'è¥ä¸šæ”¶å…¥': 'revenue',
                'è¥ä¸šæ”¶å…¥(ä¸‡å…ƒ)': 'revenue',
                'è¥ä¸šæ€»æ”¶å…¥åŒæ¯”å¢é•¿': 'revenue_yoy',
                'è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡': 'revenue_yoy',
                'ROE': 'roe',
                'å‡€èµ„äº§æ”¶ç›Šç‡': 'roe',
                'åŠ æƒå‡€èµ„äº§æ”¶ç›Šç‡': 'roe',
                'åŸºæœ¬æ¯è‚¡æ”¶ç›Š': 'eps_basic',
                'æ¯è‚¡æ”¶ç›Š': 'eps_basic',
                'åŸºæœ¬æ¯è‚¡æ”¶ç›Š(å…ƒ)': 'eps_basic',
                'æ‘Šè–„æ¯è‚¡æ”¶ç›Š': 'eps_basic'
            }
            
            # ç­›é€‰å¯ç”¨å­—æ®µ
            available_mappings = {}
            for orig_col in financial_data.columns:
                if orig_col in column_mappings:
                    available_mappings[orig_col] = column_mappings[orig_col]
            
            if len(available_mappings) == 0:
                logger.warning(f"è‚¡ç¥¨ {code} æœªæ‰¾åˆ°åŒ¹é…çš„è´¢åŠ¡æŒ‡æ ‡åˆ—")
                return
            
            # ä¿ç•™éœ€è¦çš„åˆ—
            keep_cols = ['date', 'code'] + list(available_mappings.keys())
            financial_data = financial_data[keep_cols]
            
            # é‡å‘½ååˆ—
            for old_name, new_name in available_mappings.items():
                if old_name in financial_data.columns:
                    financial_data[new_name] = pd.to_numeric(
                        financial_data[old_name], errors='coerce'
                    )
            
            # æ•°æ®ç±»å‹ä¼˜åŒ–
            if LIGHT_CONFIG['dtype_optimization'] == 'float32':
                standard_cols = list(available_mappings.values())
                for col in standard_cols:
                    if col in financial_data.columns:
                        financial_data[col] = financial_data[col].astype('float32')
            
            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
            start_dt = pd.to_datetime(self.start_date)
            end_dt = pd.to_datetime(self.end_date)
            financial_data = financial_data[
                (financial_data['date'] >= start_dt) & 
                (financial_data['date'] <= end_dt)
            ]
            
            if financial_data.empty:
                return
            
            # ä¿å­˜ä¸ºParquetåˆ†åŒº
            stock_dir = self.fundamental_parquet_dir / f'code={code}'
            stock_dir.mkdir(exist_ok=True)
            
            # å†™å…¥Parquetæ–‡ä»¶
            parquet_file = stock_dir / f'part-{code}.parquet'
            financial_data.to_parquet(parquet_file, index=False, compression='snappy')
            
        except Exception as e:
            logger.warning(f"è‚¡ç¥¨ {code} åŸºæœ¬é¢æ•°æ®å¤„ç†å¤±è´¥: {e}")
    
    def _load_fundamental_parquet(self, 
                                 start_date: Optional[str] = None,
                                 end_date: Optional[str] = None) -> pd.DataFrame:
        """
        åŠ è½½Parquetåˆ†åŒºçš„åŸºæœ¬é¢æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸè¿‡æ»¤
            end_date: ç»“æŸæ—¥æœŸè¿‡æ»¤
            
        Returns:
            åŸºæœ¬é¢æ•°æ®DataFrame
        """
        if not self.fundamental_parquet_dir.exists():
            logger.warning("ParquetåŸºæœ¬é¢æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return pd.DataFrame()
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = []
        if start_date:
            filters.append(('date', '>', pd.to_datetime(start_date)))
        if end_date:
            filters.append(('date', '<=', pd.to_datetime(end_date)))
        
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨parquetæ–‡ä»¶
            parquet_files = list(self.fundamental_parquet_dir.glob('**/*.parquet'))
            if not parquet_files:
                logger.info("æœªæ‰¾åˆ°Parquetæ–‡ä»¶ï¼Œè¿”å›ç©ºDataFrame")
                return pd.DataFrame()
            
            # è¯»å–æ‰€æœ‰åˆ†åŒºæ•°æ®
            dataset = pq.ParquetDataset(
                self.fundamental_parquet_dir,
                filters=filters if filters else None
            )
            
            table = dataset.read()
            df = table.to_pandas()
            
            logger.info(f"ä»ParquetåŠ è½½åŸºæœ¬é¢æ•°æ®: {len(df)}æ¡è®°å½•")
            return df
            
        except Exception as e:
            logger.error(f"åŠ è½½ParquetåŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def rolling_train(self, 
                     window_years: int = 3,
                     step_years: int = 1,
                     dtype: str = 'float32') -> Dict[str, str]:
        """
        æ»šåŠ¨çª—å£è®­ç»ƒ
        
        Args:
            window_years: è®­ç»ƒçª—å£å¹´æ•°
            step_years: æ­¥é•¿å¹´æ•°
            dtype: æ•°æ®ç±»å‹ä¼˜åŒ–
            
        Returns:
            è®­ç»ƒæ¨¡å‹æ–‡ä»¶è·¯å¾„å­—å…¸
        """
        logger.info(f"ğŸ”„ å¼€å§‹æ»šåŠ¨è®­ç»ƒï¼Œçª—å£: {window_years}å¹´ï¼Œæ­¥é•¿: {step_years}å¹´")
        self._log_memory_usage("æ»šåŠ¨è®­ç»ƒå¼€å§‹")
        
        # ç”Ÿæˆæ‰€æœ‰æ»šåŠ¨çª—å£
        windows = self._generate_rolling_windows(window_years, step_years)
        logger.info(f"ç”Ÿæˆ {len(windows)} ä¸ªè®­ç»ƒçª—å£")
        
        model_paths = {}
        
        for i, (train_start, train_end) in enumerate(windows):
            logger.info(f"ğŸ“ˆ è®­ç»ƒçª—å£ {i+1}/{len(windows)}: {train_start} - {train_end}")
            
            try:
                # ä¸´æ—¶è°ƒæ•´æ—¥æœŸèŒƒå›´
                original_start = self.start_date
                original_end = self.end_date
                
                self.start_date = train_start
                self.end_date = train_end
                
                # å‡†å¤‡çª—å£æ•°æ®é›†
                self._log_memory_usage(f"å‡†å¤‡çª—å£{i+1}æ•°æ®é›†")
                dataset = self._prepare_dataset_optimized(dtype=dtype, drop_raw=True)
                
                # è®­ç»ƒæ¨¡å‹
                self._log_memory_usage(f"è®­ç»ƒçª—å£{i+1}æ¨¡å‹")
                models = self.train_model(dataset)
                
                # ä¿å­˜æ¨¡å‹
                model_name = f"lowprice_{train_start[:4]}_{train_end[:4]}.pkl"
                model_path = self.model_dir / model_name
                
                with open(model_path, 'wb') as f:
                    pickle.dump(models['lgb'], f)
                
                model_paths[f"{train_start}_{train_end}"] = str(model_path)
                logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
                
                # å¼ºåˆ¶æ¸…ç†å†…å­˜
                del dataset, models
                self._force_gc()
                self._log_memory_usage(f"çª—å£{i+1}è®­ç»ƒå®Œæˆ")
                
                # æ¢å¤åŸå§‹æ—¥æœŸèŒƒå›´
                self.start_date = original_start
                self.end_date = original_end
                
            except Exception as e:
                logger.error(f"çª—å£ {train_start}-{train_end} è®­ç»ƒå¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… æ»šåŠ¨è®­ç»ƒå®Œæˆï¼Œå…±è®­ç»ƒ {len(model_paths)} ä¸ªæ¨¡å‹")
        
        # ä¿å­˜æ¨¡å‹ç´¢å¼•
        model_index_path = self.model_dir / 'rolling_models_index.json'
        with open(model_index_path, 'w') as f:
            json.dump(model_paths, f, indent=2)
        
        return model_paths
    
    def _generate_rolling_windows(self, window_years: int, step_years: int) -> List[Tuple[str, str]]:
        """ç”Ÿæˆæ»šåŠ¨çª—å£åˆ—è¡¨"""
        windows = []
        
        start_dt = datetime.strptime(self.start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(self.end_date, '%Y-%m-%d')
        
        current_start = start_dt
        
        while True:
            # è®¡ç®—çª—å£ç»“æŸæ—¶é—´
            window_end = current_start + timedelta(days=window_years * 365)
            
            # å¦‚æœçª—å£ç»“æŸæ—¶é—´è¶…è¿‡æ€»ç»“æŸæ—¶é—´ï¼Œè°ƒæ•´ä¸ºæ€»ç»“æŸæ—¶é—´
            if window_end > end_dt:
                window_end = end_dt
            
            # æ·»åŠ çª—å£
            windows.append((
                current_start.strftime('%Y-%m-%d'),
                window_end.strftime('%Y-%m-%d')
            ))
            
            # å¦‚æœå·²ç»åˆ°è¾¾æ€»ç»“æŸæ—¶é—´ï¼Œåœæ­¢
            if window_end >= end_dt:
                break
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªçª—å£
            current_start = current_start + timedelta(days=step_years * 365)
        
        return windows
    
    def _prepare_dataset_optimized(self, dtype: str = 'float32', drop_raw: bool = True):
        """
        å†…å­˜ä¼˜åŒ–çš„æ•°æ®é›†å‡†å¤‡
        
        Args:
            dtype: æ•°æ®ç±»å‹
            drop_raw: æ˜¯å¦åˆ é™¤åŸå§‹æ•°æ®ä»¥èŠ‚çœå†…å­˜
            
        Returns:
            ä¼˜åŒ–çš„æ•°æ®é›†
        """
        logger.info("ğŸ“Š å‡†å¤‡ä¼˜åŒ–æ•°æ®é›†...")
        self._log_memory_usage("å¼€å§‹æ•°æ®é›†ä¼˜åŒ–")
        
        # è·å–åŸºæœ¬é¢æ•°æ®ï¼ˆæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼‰
        fundamental_data = self._load_fundamental_parquet(
            start_date=self.start_date,
            end_date=self.end_date
        )
        
        if not fundamental_data.empty:
            fundamental_with_metrics = self._calculate_rolling_metrics(fundamental_data)
            if drop_raw:
                del fundamental_data
                gc.collect()
        else:
            fundamental_with_metrics = pd.DataFrame()
            logger.warning("åŸºæœ¬é¢æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŸºæœ¬é¢ç‰¹å¾è®¡ç®—")
        
        # ç²¾ç®€æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
        essential_features = [
            # æ ¸å¿ƒä»·æ ¼åŠ¨é‡ï¼ˆå¿…éœ€ï¼‰
            "Rsi()",
            "($close-Ref($close,20))/Ref($close,20)",
            "($close-Ref($close,5))/Ref($close,5)",
            
            # æ ¸å¿ƒä¼°å€¼ï¼ˆå¿…éœ€ï¼‰
            "PeTtm()",
            "PbMrq()",
            "RoeTtm()",
            
            # ä½ä»·æ ‡è®°ï¼ˆæ ¸å¿ƒï¼‰
            "If($close <= 20, 1, 0)",
            
            # æˆé•¿æ€§æŒ‡æ ‡
            "($close / Ref($close, 63) - 1)",
        ]
        
        # 63æ—¥æ”¶ç›Šæ ‡ç­¾
        label_expr = "Ref($close, -63) / Ref($close, -1) - 1"
        
        # ä½¿ç”¨ä¼˜åŒ–çš„handleré…ç½®
        try:
            from qlib.contrib.data.handler import Alpha158
            
            # å†…å­˜ä¼˜åŒ–çš„é¢„å¤„ç†å™¨
            optimized_processors = []
            if dtype == 'float32':
                optimized_processors.append({
                    "class": "Fillna", 
                    "kwargs": {"fields_group": "feature"}
                })
            else:
                optimized_processors.extend([
                    {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature", "clip_outlier": True}},
                    {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
                ])
            
            handler = Alpha158(
                instruments=self.universe,
                start_time=self.start_date,
                end_time=self.end_date,
                fit_start_time=self.start_date,
                fit_end_time=self.end_date,
                infer_processors=optimized_processors,
                learn_processors=[
                    {"class": "DropnaLabel"},
                    {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
                ],
            )
            
            # åˆ†æ®µé…ç½®
            mid_date = (datetime.strptime(self.start_date, '%Y-%m-%d') + 
                       timedelta(days=(datetime.strptime(self.end_date, '%Y-%m-%d') - 
                                     datetime.strptime(self.start_date, '%Y-%m-%d')).days * 0.8)).strftime('%Y-%m-%d')
            
            segments = {
                "train": (self.start_date, mid_date),
                "valid": (mid_date, self.end_date),
                "test": (mid_date, self.end_date),
            }
            
            # åˆ›å»ºæ•°æ®é›†
            self._log_memory_usage("å¼€å§‹åˆ›å»ºä¼˜åŒ–DatasetH")
            from qlib.data.dataset import DatasetH
            dataset = DatasetH(handler, segments)
            self._log_memory_usage("ä¼˜åŒ–DatasetHåˆ›å»ºå®Œæˆ")
            
            return dataset
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
            # å›é€€åˆ°çˆ¶ç±»æ–¹æ³•
            return super()._prepare_dataset()
    
    def predict_latest_window(self) -> Dict[str, pd.DataFrame]:
        """
        ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†
        
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        logger.info("ğŸ”® ä½¿ç”¨æœ€æ–°çª—å£æ¨¡å‹è¿›è¡Œæ¨ç†...")
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
        model_files = list(self.model_dir.glob('lowprice_*.pkl'))
        if not model_files:
            raise FileNotFoundError("æœªæ‰¾åˆ°æ»šåŠ¨è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ --rolling_train")
        
        # æ‰¾åˆ°æœ€æ–°çš„æ¨¡å‹ï¼ˆæŒ‰æ–‡ä»¶åä¸­çš„å¹´ä»½æ’åºï¼‰
        latest_model = max(model_files, key=lambda x: x.stem.split('_')[-1])
        logger.info(f"ä½¿ç”¨æœ€æ–°æ¨¡å‹: {latest_model}")
        
        # åŠ è½½æ¨¡å‹
        with open(latest_model, 'rb') as f:
            model = pickle.load(f)
        
        models = {'lgb': model}
        
        # å‡†å¤‡æœ€æ–°æ•°æ®è¿›è¡Œæ¨ç†
        self._log_memory_usage("å‡†å¤‡æ¨ç†æ•°æ®")
        
        # ä½¿ç”¨æœ€è¿‘2å¹´æ•°æ®è¿›è¡Œæ¨ç†
        inference_start = (datetime.now() - timedelta(days=2*365)).strftime('%Y-%m-%d')
        inference_end = self.end_date
        
        original_start = self.start_date
        self.start_date = inference_start
        
        try:
            dataset = self._prepare_dataset_optimized(drop_raw=True)
            
            # æ‰§è¡Œæ¨ç†
            self._log_memory_usage("å¼€å§‹æ¨¡å‹æ¨ç†")
            pred = model.predict(dataset)
            pred_df = pred.reset_index()
            pred_df.columns = ['date', 'instrument', 'prediction']
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            output_path = self.output_dir / 'signal_latest_window.csv'
            pred_df.to_csv(output_path, index=False)
            logger.info(f"ğŸ’¾ æœ€æ–°æ¨ç†ç»“æœä¿å­˜è‡³: {output_path}")
            
            self._log_memory_usage("æ¨ç†å®Œæˆ")
            
            return {'latest': pred_df}
            
        finally:
            self.start_date = original_start
    
    def get_memory_summary(self) -> Dict:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()
            
            return {
                'current_mb': memory_info.rss / 1024 / 1024,
                'peak_mb': memory_info.peak_wss / 1024 / 1024 if hasattr(memory_info, 'peak_wss') else 0,
                'percent': process.memory_percent(),
                'limit_gb': self.max_ram_gb,
                'status': 'OK' if memory_info.rss / 1024 / 1024 / 1024 < self.max_ram_gb else 'WARNING'
            }
        except:
            return {'status': 'UNKNOWN'}


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä½ä»·+ç¨³å®šå¢é•¿é€‰è‚¡å™¨ - å†…å­˜å‹å¥½ç‰ˆæœ¬')
    
    # ä¸»è¦æ“ä½œ
    parser.add_argument('--rolling_train', action='store_true', help='æ»šåŠ¨çª—å£è®­ç»ƒ')
    parser.add_argument('--predict_latest', action='store_true', help='ä½¿ç”¨æœ€æ–°æ¨¡å‹æ¨ç†')
    parser.add_argument('--backtest_stocks', action='store_true', help='å›æµ‹')
    
    # æ»šåŠ¨è®­ç»ƒå‚æ•°
    parser.add_argument('--window_years', type=int, default=3, help='è®­ç»ƒçª—å£å¹´æ•°')
    parser.add_argument('--step_years', type=int, default=1, help='æ­¥é•¿å¹´æ•°')
    parser.add_argument('--dtype', type=str, default='float32', 
                       choices=['float32', 'float64'], help='æ•°æ®ç±»å‹ä¼˜åŒ–')
    
    # å†…å­˜ç®¡ç†
    parser.add_argument('--max_ram_gb', type=int, default=4, help='æœ€å¤§å†…å­˜é™åˆ¶(GB)')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--topk', type=int, default=10, help='é€‰æ‹©è‚¡ç¥¨æ•°é‡')
    parser.add_argument('--start_date', type=str, default=LIGHT_CONFIG['start_date'], help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end_date', type=str, default=LIGHT_CONFIG['end_date'], help='ç»“æŸæ—¥æœŸ')
    parser.add_argument('--universe', type=str, default=LIGHT_CONFIG['universe'], help='è‚¡ç¥¨æ± ')
    
    args = parser.parse_args()
    
    print("âœ“ å†…å­˜å‹å¥½ç‰ˆæœ¬å¯åŠ¨")
    
    # åˆ›å»ºè½»é‡çº§é€‰è‚¡å™¨
    selector = LowPriceGrowthSelectorLight(
        start_date=args.start_date,
        end_date=args.end_date,
        universe=args.universe,
        max_ram_gb=args.max_ram_gb
    )
    
    try:
        if args.rolling_train:
            # é¦–å…ˆè·å–åŸºæœ¬é¢æ•°æ®
            selector.fetch_fundamental_data_parquet()
            
            # æ‰§è¡Œæ»šåŠ¨è®­ç»ƒ
            model_paths = selector.rolling_train(
                window_years=args.window_years,
                step_years=args.step_years,
                dtype=args.dtype
            )
            
            print(f"\n=== æ»šåŠ¨è®­ç»ƒå®Œæˆ ===")
            print(f"è®­ç»ƒæ¨¡å‹æ•°é‡: {len(model_paths)}")
            for window, path in model_paths.items():
                print(f"  {window}: {path}")
        
        if args.predict_latest:
            predictions = selector.predict_latest_window()
            latest_pred = predictions['latest']
            print(f"\n=== æœ€æ–°æ¨ç†å®Œæˆ ===")
            print(f"é¢„æµ‹æ ·æœ¬æ•°: {len(latest_pred)}")
            print(f"æ—¶é—´èŒƒå›´: {latest_pred['date'].min()} - {latest_pred['date'].max()}")
        
        if args.backtest_stocks:
            # ä½¿ç”¨çˆ¶ç±»çš„å›æµ‹æ–¹æ³•
            equity_curve, metrics = selector.backtest(topk=args.topk)
            
            print(f"\n=== å›æµ‹ç»“æœ ===")
            for key, value in metrics.items():
                if isinstance(value, float):
                    print(f"{key}: {value:.4f}")
                else:
                    print(f"{key}: {value}")
        
        # æ˜¾ç¤ºå†…å­˜æ‘˜è¦
        memory_summary = selector.get_memory_summary()
        print(f"\n=== å†…å­˜ä½¿ç”¨æ‘˜è¦ ===")
        print(f"å½“å‰å†…å­˜: {memory_summary.get('current_mb', 0):.1f} MB")
        print(f"å†…å­˜å ç”¨: {memory_summary.get('percent', 0):.1f}%")
        print(f"å†…å­˜çŠ¶æ€: {memory_summary.get('status', 'UNKNOWN')}")
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()