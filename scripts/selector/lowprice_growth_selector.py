#!/usr/bin/env python3
"""
ä½ä»·+ç¨³å®šå¢é•¿é€‰è‚¡å™¨ (Low Price + Stable Growth Stock Selector)
================================================================

åŸºäºQlib + AkShareçš„æ·±åº¦åŸºæœ¬é¢åˆ†æé€‰è‚¡ç³»ç»Ÿï¼Œä¸“æ³¨äºä½ä»·ç¨³å®šå¢é•¿è‚¡ç¥¨ï¼Œç‰¹æ€§ï¼š
- ğŸ·ï¸ ä½ä»·æ ‡å‡†: æ”¶ç›˜ä»· â‰¤ 20å…ƒ
- ğŸ“ˆ ç¨³å®šå¢é•¿: 3å¹´EPS CAGR > 15%, ROEå‡å€¼ > 12%
- ğŸ¯ ä»·å€¼æŠ•èµ„: PEG < 1.2, ROEæ ‡å‡†å·® < 3%
- ğŸ“Š æ·±åº¦åŸºæœ¬é¢: å­£åº¦è´¢åŠ¡æ•°æ®æ•´åˆä¸æ»šåŠ¨æŒ‡æ ‡è®¡ç®—
- ğŸ”„ å­£åº¦è°ƒä»“: é»˜è®¤å­£åº¦è°ƒä»“é¢‘ç‡ï¼Œé€‚åº”åŸºæœ¬é¢å˜åŒ–
- âš–ï¸ åŸºæœ¬é¢æƒé‡: åŸºäºEPS_CAGR * ROE_mean / PEGçš„æƒé‡åˆ†é…

Usage:
    # å®Œæ•´æµç¨‹ï¼šæ•°æ®å‡†å¤‡ -> è®­ç»ƒ -> å›æµ‹
    python lowprice_growth_selector.py --prepare_data --train --backtest_stocks

    # ä½ä»·æˆé•¿è‚¡é€‰æ‹©ï¼ˆä¸¥æ ¼ç­›é€‰ï¼‰
    python lowprice_growth_selector.py --backtest_stocks \\
        --topk 15 --price_max 20 --peg_max 1.1 --roe_min 0.15

    # åŸºæœ¬é¢æƒé‡åˆ†é…
    python lowprice_growth_selector.py --backtest_stocks \\
        --topk 12 --weight_method fundamental

    # å­£åº¦è°ƒä»“å›æµ‹
    python lowprice_growth_selector.py --backtest_stocks \\
        --topk 10 --rebalance_freq Q --weight_method risk_opt

æ ¸å¿ƒæ”¹è¿›:
1. 63äº¤æ˜“æ—¥æ ‡ç­¾ï¼ˆâ‰ˆ3ä¸ªæœˆæ”¶ç›Šï¼‰åŒ¹é…å­£åº¦åŸºæœ¬é¢
2. æ»šåŠ¨3å¹´EPS/Revenue CAGRå’ŒROEç»Ÿè®¡æŒ‡æ ‡
3. ä½ä»·æ ‡è®°(LowPriceFlag)å¼ºåˆ¶ç­›é€‰
4. PEGä¼°å€¼ä¸ROEç¨³å®šæ€§çº¦æŸ
5. åŸºæœ¬é¢é©±åŠ¨çš„æƒé‡åˆ†é…ç®—æ³•
6. å­£åº¦è°ƒä»“ä¼˜åŒ–æ¢æ‰‹ç‡æ§åˆ¶
"""

import os
import sys
import argparse
import warnings
import logging
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
from datetime import datetime, timedelta
import json
from tqdm import tqdm
import gc
import psutil
import resource

# è®¾ç½®è­¦å‘Šå’Œéšæœºç§å­
warnings.filterwarnings('ignore')
np.random.seed(42)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ä¾èµ–æ£€æŸ¥å’Œå¯¼å…¥
try:
    import qlib
    from qlib.config import REG_CN
    from qlib.utils import init_instance_by_config
    from qlib.data import D
    from qlib.data.dataset import DatasetH
    from qlib.data.dataset.handler import DataHandlerLP
    from qlib.contrib.model.gbdt import LGBModel
    from qlib.contrib.strategy.signal_strategy import TopkDropoutStrategy
except ImportError as e:
    logger.error(f"Qlibå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install pyqlib")
    sys.exit(1)

try:
    import akshare as ak
except ImportError as e:
    logger.error(f"AkShareå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install akshare>=1.11")
    sys.exit(1)

try:
    import lightgbm as lgb
    from sklearn.model_selection import TimeSeriesSplit
    from sklearn.metrics import mean_squared_error
except ImportError as e:
    logger.error(f"æœºå™¨å­¦ä¹ åº“å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install lightgbm scikit-learn")
    sys.exit(1)

try:
    import cvxpy as cp
except ImportError as e:
    logger.warning(f"CVXPYå¯¼å…¥å¤±è´¥: {e}")
    print("ä¼˜åŒ–æƒé‡åŠŸèƒ½å—é™ï¼Œå»ºè®®å®‰è£…: pip install cvxpy")

# é…ç½®å‚æ•°
CONFIG = {
    'data_path': '~/.qlib/qlib_data/cn_data',
    'model_path': './models',
    'output_path': './output',
    'start_date': '2014-01-01',
    'end_date': '2025-06-04',
    'train_end': '2023-12-31',
    'universe': 'all',
    'random_seed': 42,
    'n_jobs': 8,

    # ä½ä»·æˆé•¿ç­–ç•¥ç‰¹å®šå‚æ•°
    'low_price_threshold': 20.0,      # ä½ä»·é˜ˆå€¼
    'peg_max_default': 1.2,           # PEGä¸Šé™
    'roe_min_default': 0.12,          # ROEä¸‹é™
    'roe_std_max_default': 0.03,      # ROEæ ‡å‡†å·®ä¸Šé™
    'eps_cagr_min': 0.15,             # EPS CAGRæœ€å°å€¼
    'lookback_years': 3,              # æ»šåŠ¨è®¡ç®—å¹´æ•°
    'rebalance_freq_default': 'Q',    # é»˜è®¤å­£åº¦è°ƒä»“
    'max_turnover': 0.5,              # æ¢æ‰‹ç‡ä¸Šé™
    'max_memory_gb': 8,               # æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶(GB)
    'memory_check_interval': 50,      # å†…å­˜æ£€æŸ¥é—´éš”
}


class LowPriceGrowthSelector:
    """ä½ä»·+ç¨³å®šå¢é•¿é€‰è‚¡å™¨"""

    def __init__(self,
                 start_date: str = CONFIG['start_date'],
                 end_date: str = CONFIG['end_date'],
                 universe: str = CONFIG['universe']):
        """
        åˆå§‹åŒ–é€‰è‚¡å™¨

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            universe: è‚¡ç¥¨æ± èŒƒå›´
        """
        self.start_date = start_date
        self.end_date = end_date
        self.universe = universe

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(CONFIG['output_path'])
        self.output_dir.mkdir(exist_ok=True)
        self.model_dir = Path(CONFIG['model_path'])
        self.model_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–Qlib
        try:
            qlib.init(provider_uri=CONFIG['data_path'], region=REG_CN)
            logger.info("âœ“ Qlibåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Qlibåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

        # å†…å­˜ç®¡ç†
        self._set_memory_limit()
        self._memory_check_counter = 0
        self._log_memory_usage("åˆå§‹åŒ–")

        # åŸºæœ¬é¢æ•°æ®ç¼“å­˜
        self.fundamental_data = None
        self.fundamental_path = self.output_dir / 'fundamental.pkl'

        logger.info(f"ä½ä»·æˆé•¿é€‰è‚¡å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"æ•°æ®èŒƒå›´: {start_date} è‡³ {end_date}")
        logger.info(f"è‚¡ç¥¨æ± : {universe}")

    def _set_memory_limit(self):
        """è®¾ç½®å†…å­˜é™åˆ¶"""
        try:
            # è®¾ç½®è½¯é™åˆ¶ä¸ºé…ç½®çš„æœ€å¤§å†…å­˜
            max_memory_bytes = CONFIG['max_memory_gb'] * 1024 * 1024 * 1024
            resource.setrlimit(resource.RLIMIT_AS, (max_memory_bytes, resource.RLIM_INFINITY))
            logger.info(f"è®¾ç½®å†…å­˜è½¯é™åˆ¶: {CONFIG['max_memory_gb']} GB")
        except Exception as e:
            logger.warning(f"æ— æ³•è®¾ç½®å†…å­˜é™åˆ¶: {e}")

    def _log_memory_usage(self, stage: str):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            memory_percent = process.memory_percent()

            # ç³»ç»Ÿå¯ç”¨å†…å­˜
            system_memory = psutil.virtual_memory()
            available_gb = system_memory.available / 1024 / 1024 / 1024

            logger.info(f"å†…å­˜ä½¿ç”¨ [{stage}]: {memory_mb:.1f} MB ({memory_percent:.1f}%), ç³»ç»Ÿå¯ç”¨: {available_gb:.1f} GB")

            # å¦‚æœå†…å­˜ä½¿ç”¨è¶…è¿‡é™åˆ¶ï¼Œè§¦å‘åƒåœ¾å›æ”¶
            if memory_mb > CONFIG['max_memory_gb'] * 1024 * 0.8:  # 80%é˜ˆå€¼
                logger.warning(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({memory_mb:.1f} MB)ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶...")
                self._force_gc()

        except Exception as e:
            logger.warning(f"å†…å­˜ç›‘æ§å¤±è´¥: {e}")

    def _force_gc(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        gc.collect()
        # æ¸…ç†numpyç¼“å­˜
        if hasattr(np, 'get_printoptions'):
            np.random.seed(42)  # é‡ç½®éšæœºç§å­
        logger.info("åƒåœ¾å›æ”¶å®Œæˆ")

    def _check_memory_periodically(self):
        """å®šæœŸæ£€æŸ¥å†…å­˜"""
        self._memory_check_counter += 1
        if self._memory_check_counter % CONFIG['memory_check_interval'] == 0:
            self._log_memory_usage(f"æ£€æŸ¥ç‚¹_{self._memory_check_counter}")

    def fetch_fundamental_data(self, force_refresh: bool = False) -> pd.DataFrame:
        """
        è·å–åŸºæœ¬é¢æ•°æ®

        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ•°æ®

        Returns:
            åŸºæœ¬é¢æ•°æ®DataFrame
        """
        logger.info("å¼€å§‹è·å–åŸºæœ¬é¢æ•°æ®...")

        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and self.fundamental_path.exists():
            logger.info("åŠ è½½ç¼“å­˜çš„åŸºæœ¬é¢æ•°æ®...")
            with open(self.fundamental_path, 'rb') as f:
                self.fundamental_data = pickle.load(f)
            logger.info(f"ç¼“å­˜æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(self.fundamental_data)}æ¡è®°å½•")
            return self.fundamental_data

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        try:
            if self.universe == 'all':
                # ç›´æ¥ä½¿ç”¨é¢„å®šä¹‰çš„è‚¡ç¥¨æ± é¿å…D.instrumentså¤æ‚æ€§
                stock_codes = [
                    'SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002',
                    'SZ300015', 'SZ002415', 'SH600276', 'SH600030', 'SZ000858',
                    'SH600276', 'SH600585', 'SH601318', 'SZ000858', 'SZ002594'
                ]
                logger.info(f"ä½¿ç”¨é¢„å®šä¹‰è‚¡ç¥¨æ± : {len(stock_codes)}åªè‚¡ç¥¨")
            else:
                # å°è¯•è·å–è‚¡ç¥¨åˆ—è¡¨
                from qlib.data import D
                instruments = D.list_instruments(instruments=self.universe, start_time=self.start_date, end_time=self.end_date)

                # å¤„ç†è¿”å›ç»“æœå¯èƒ½çš„ä¸åŒæ ¼å¼
                if isinstance(instruments, str):
                    # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨é»˜è®¤
                    logger.warning("D.list_instrumentsè¿”å›å­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
                    stock_codes = ['SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002']
                elif isinstance(instruments, (list, tuple)) and len(instruments) > 0:
                    stock_codes = [inst for inst in instruments if isinstance(inst, str) and inst.startswith(('000', '002', '003', '300', '600', '601', '603', '688'))]
                else:
                    stock_codes = ['SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002']

                logger.info(f"è·å–åˆ°{len(stock_codes)}åªè‚¡ç¥¨")

            if len(stock_codes) == 0:
                logger.warning("æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± ")
                stock_codes = ['SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002']

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± 
            stock_codes = ['SH600000', 'SH600036', 'SH600519', 'SZ000001', 'SZ000002']
            logger.info(f"ä½¿ç”¨é»˜è®¤è‚¡ç¥¨æ± : {len(stock_codes)}åªè‚¡ç¥¨")

        fundamental_data = []
        failed_stocks = []

        # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
        for i, code in enumerate(tqdm(stock_codes, desc="è·å–åŸºæœ¬é¢æ•°æ®")):
            try:
                # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼ (SH600000 -> 600000)
                ak_code = code.replace('SH', '').replace('SZ', '')

                # è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ®
                try:
                    financial_data = ak.stock_financial_analysis_indicator_em(symbol=ak_code)
                    if financial_data.empty:
                        continue

                    # æ•°æ®æ¸…ç†å’Œæ ¼å¼åŒ–
                    financial_data['date'] = pd.to_datetime(financial_data['date'])
                    financial_data['code'] = code

                    # åªä¿ç•™éœ€è¦çš„å­—æ®µ
                    required_fields = ['date', 'code', 'å‡€åˆ©æ¶¦', 'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿', 'è¥ä¸šæ€»æ”¶å…¥',
                                     'è¥ä¸šæ€»æ”¶å…¥åŒæ¯”å¢é•¿', 'ROE', 'åŸºæœ¬æ¯è‚¡æ”¶ç›Š']
                    available_fields = ['date', 'code'] + [f for f in required_fields[2:] if f in financial_data.columns]

                    if len(available_fields) > 2:
                        financial_data = financial_data[available_fields]
                        fundamental_data.append(financial_data)

                except Exception as e:
                    # å°è¯•å¤‡ç”¨æ•°æ®æº
                    try:
                        report_data = ak.stock_financial_report_sina(stock=ak_code, symbol='è´Ÿå€ºè¡¨')
                        if not report_data.empty:
                            report_data['code'] = code
                            fundamental_data.append(report_data)
                    except:
                        failed_stocks.append(code)
                        continue

                # æ§åˆ¶APIè°ƒç”¨é¢‘ç‡
                if i % 100 == 0 and i > 0:
                    logger.info(f"å·²å¤„ç† {i}/{len(stock_codes)} åªè‚¡ç¥¨")

            except Exception as e:
                failed_stocks.append(code)
                continue

        if fundamental_data:
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            self.fundamental_data = pd.concat(fundamental_data, ignore_index=True)

            # æ ‡å‡†åŒ–åˆ—å
            column_mapping = {
                'å‡€åˆ©æ¶¦': 'net_profit',
                'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿': 'net_profit_yoy',
                'è¥ä¸šæ€»æ”¶å…¥': 'revenue',
                'è¥ä¸šæ€»æ”¶å…¥åŒæ¯”å¢é•¿': 'revenue_yoy',
                'ROE': 'roe',
                'åŸºæœ¬æ¯è‚¡æ”¶ç›Š': 'eps_basic'
            }

            for old_name, new_name in column_mapping.items():
                if old_name in self.fundamental_data.columns:
                    self.fundamental_data[new_name] = pd.to_numeric(
                        self.fundamental_data[old_name], errors='coerce'
                    )

            # è¿‡æ»¤æ—¥æœŸèŒƒå›´
            start_dt = pd.to_datetime(self.start_date)
            end_dt = pd.to_datetime(self.end_date)
            self.fundamental_data = self.fundamental_data[
                (self.fundamental_data['date'] >= start_dt) &
                (self.fundamental_data['date'] <= end_dt)
            ]

            # ä¿å­˜ç¼“å­˜
            with open(self.fundamental_path, 'wb') as f:
                pickle.dump(self.fundamental_data, f)

            logger.info(f"åŸºæœ¬é¢æ•°æ®è·å–å®Œæˆï¼š{len(self.fundamental_data)}æ¡è®°å½•")
            logger.info(f"å¤±è´¥è‚¡ç¥¨æ•°ï¼š{len(failed_stocks)}")

        else:
            logger.warning("æœªè·å–åˆ°ä»»ä½•åŸºæœ¬é¢æ•°æ®")
            self.fundamental_data = pd.DataFrame()

        return self.fundamental_data

    def _calculate_rolling_metrics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—æ»šåŠ¨æŒ‡æ ‡

        Args:
            df: åŸºæœ¬é¢æ•°æ®

        Returns:
            åŒ…å«æ»šåŠ¨æŒ‡æ ‡çš„æ•°æ®
        """
        result_data = []

        for code in df['code'].unique():
            stock_data = df[df['code'] == code].sort_values('date').copy()

            if len(stock_data) < 8:  # éœ€è¦è‡³å°‘2å¹´çš„å­£åº¦æ•°æ®
                continue

            # è®¡ç®—æ»šåŠ¨3å¹´EPS CAGR
            if 'eps_basic' in stock_data.columns:
                stock_data['EPS_3Y_CAGR'] = stock_data['eps_basic'].rolling(
                    window=12, min_periods=8
                ).apply(lambda x: self._calculate_cagr(x), raw=False)

            # è®¡ç®—æ»šåŠ¨3å¹´Revenue CAGR
            if 'revenue' in stock_data.columns:
                stock_data['Revenue_3Y_CAGR'] = stock_data['revenue'].rolling(
                    window=12, min_periods=8
                ).apply(lambda x: self._calculate_cagr(x), raw=False)

            # è®¡ç®—æ»šåŠ¨3å¹´ROEç»Ÿè®¡
            if 'roe' in stock_data.columns:
                stock_data['ROE_mean_3Y'] = stock_data['roe'].rolling(
                    window=12, min_periods=8
                ).mean()

                stock_data['ROE_std_3Y'] = stock_data['roe'].rolling(
                    window=12, min_periods=8
                ).std()

            result_data.append(stock_data)

        if result_data:
            return pd.concat(result_data, ignore_index=True)
        else:
            return pd.DataFrame()

    def _calculate_cagr(self, series: pd.Series) -> float:
        """è®¡ç®—å¤åˆå¹´å¢é•¿ç‡"""
        try:
            if len(series) < 2:
                return np.nan

            start_val = series.iloc[0]
            end_val = series.iloc[-1]

            if start_val <= 0 or end_val <= 0:
                return np.nan

            years = len(series) / 4.0  # å­£åº¦æ•°æ®è½¬å¹´æ•°
            cagr = (end_val / start_val) ** (1/years) - 1

            return cagr if not np.isinf(cagr) else np.nan

        except:
            return np.nan

    def _prepare_dataset(self) -> DatasetH:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›†ï¼ˆæ‰©å±•åŸºæœ¬é¢ç‰¹å¾ï¼‰

        Returns:
            Qlibæ•°æ®é›†å¯¹è±¡
        """
        logger.info("å‡†å¤‡æ•°æ®é›†...")
        self._log_memory_usage("å¼€å§‹å‡†å¤‡æ•°æ®é›†")

        # è·å–åŸºæœ¬é¢æ•°æ®
        if self.fundamental_data is None:
            self.fetch_fundamental_data()
            self._log_memory_usage("åŸºæœ¬é¢æ•°æ®è·å–å®Œæˆ")

        # è®¡ç®—æ»šåŠ¨æŒ‡æ ‡
        if self.fundamental_data is not None and not self.fundamental_data.empty:
            fundamental_with_metrics = self._calculate_rolling_metrics(self.fundamental_data)
        else:
            fundamental_with_metrics = pd.DataFrame()
            logger.warning("åŸºæœ¬é¢æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åŸºæœ¬é¢ç‰¹å¾è®¡ç®—")

        # åŸæœ‰æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        technical_features = [
            # ä»·æ ¼åŠ¨é‡ç±»
            "Roc(($close-Ref($close,20))/Ref($close,20),5)",     # 20æ—¥ä»·æ ¼åŠ¨é‡
            "Rsv()",                                              # RSVéšæœºå€¼
            "Rsi()",                                              # RSIç›¸å¯¹å¼ºå¼±
            "WILLR()",                                            # å¨å»‰æŒ‡æ ‡
            "Aroon()",                                            # é˜¿éš†æŒ‡æ ‡
            "Psy()",                                              # å¿ƒç†çº¿
            "BIAS()",                                             # ä¹–ç¦»ç‡
            "BOP()",                                              # å‡åŠ¿æŒ‡æ ‡
            "CCI()",                                              # å•†å“é€šé“æŒ‡æ•°

            # ä¼°å€¼è´¨é‡ç±»
            "PeTtm()",                                            # å¸‚ç›ˆç‡
            "PbMrq()",                                            # å¸‚å‡€ç‡
            "PsTtm()",                                            # å¸‚é”€ç‡
            "PcfTtm()",                                           # å¸‚ç°ç‡
            "RoeTtm()",                                           # ROE
            "RoaTtm()",                                           # ROA
            "RoicTtm()",                                          # ROIC
            "ProfitGrowthRateTtm()",                             # ç›ˆåˆ©å¢é•¿ç‡
            "RevenueGrowthRateTtm()",                            # è¥æ”¶å¢é•¿ç‡

            # æˆäº¤é‡æ³¢åŠ¨ç±»
            "VSTD(5)",                                            # 5æ—¥æˆäº¤é‡æ ‡å‡†å·®
            "VMA(5)",                                             # 5æ—¥æˆäº¤é‡å‡çº¿
            "ATR(14)",                                            # çœŸå®æ³¢åŠ¨ç‡
            "BETA(252)",                                          # Betaç³»æ•°
            "CMRA()",                                             # ç´¯ç§¯å¤šæœŸæ”¶ç›Šæ³¢åŠ¨
            "($ret_1_1m)",                                        # æœˆæ”¶ç›Šç‡
            "($ret_1_3m)",                                        # 3æœˆæ”¶ç›Šç‡
            "($ret_1_6m)",                                        # 6æœˆæ”¶ç›Šç‡
            "CORR(($ret_1_1d), Mean(($ret_1_1d), 252), 252)",    # æ”¶ç›Šç›¸å…³æ€§
        ]

        # æ–°å¢åŸºæœ¬é¢ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨Qlibå†…ç½®æ•°æ®ï¼‰
        fundamental_features = [
            # ä½ä»·æ ‡è®°
            "If($close <= 20, 1, 0)",  # LowPriceFlag

            # ç®€åŒ–åŸºæœ¬é¢æŒ‡æ ‡ï¼ˆä½¿ç”¨å·²æœ‰çš„Qlibå­—æ®µï¼‰
            "RoeTtm()",  # ROEæŒ‡æ ‡
            "PeTtm()",   # PEæŒ‡æ ‡
            "PbMrq()",   # PBæŒ‡æ ‡

            # å¢é•¿æ€§æŒ‡æ ‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            "($close / Ref($close, 63) - 1)",  # 63æ—¥ä»·æ ¼å¢é•¿
            "(Mean($volume, 20) / Mean($volume, 60))",  # æˆäº¤é‡æ¯”ç‡
        ]

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        all_features = technical_features + fundamental_features

        # ä¿®æ”¹æ ‡ç­¾ä¸º63äº¤æ˜“æ—¥æ”¶ç›Šï¼ˆâ‰ˆ3ä¸ªæœˆï¼‰
        label_expr = "Ref($close, -63) / Ref($close, -1) - 1"

        # å†…å­˜ä¼˜åŒ–ï¼šè‡ªåŠ¨é™åˆ¶æ—¶é—´èŒƒå›´
        original_start = self.start_date
        start_dt = datetime.strptime(self.start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(self.end_date, '%Y-%m-%d')
        time_span_years = (end_dt - start_dt).days / 365.25

        # å¦‚æœæ—¶é—´èŒƒå›´è¶…è¿‡3å¹´ï¼Œè‡ªåŠ¨é™åˆ¶ä¸ºæœ€è¿‘3å¹´ä»¥èŠ‚çœå†…å­˜ï¼ˆé™¤éç”¨æˆ·ç¦ç”¨ï¼‰
        if time_span_years > 3 and not getattr(self, '_disable_memory_limit', False):
            optimized_start = (end_dt - timedelta(days=3*365)).strftime('%Y-%m-%d')
            logger.warning(f"âš ï¸ æ—¶é—´èŒƒå›´è¿‡é•¿({time_span_years:.1f}å¹´)ï¼Œä¸ºèŠ‚çœå†…å­˜è‡ªåŠ¨ä¼˜åŒ–ä¸ºæœ€è¿‘3å¹´")
            logger.warning(f"åŸå§‹èŒƒå›´: {original_start} - {self.end_date}")
            logger.warning(f"ä¼˜åŒ–èŒƒå›´: {optimized_start} - {self.end_date}")
            logger.warning(f"ğŸ’¡ å¦‚éœ€ä½¿ç”¨å®Œæ•´èŒƒå›´ï¼Œè¯·æ·»åŠ  --disable_memory_limit å‚æ•°")
            self.start_date = optimized_start

        # ä½¿ç”¨ç®€åŒ–çš„æ•°æ®é›†é…ç½®ï¼Œé¿å…å¤æ‚çš„handleré…ç½®
        try:
            # å°è¯•åˆ›å»ºåŸºäºå·²æœ‰æ•°æ®çš„ç®€å•æ•°æ®é›†
            from qlib.contrib.data.handler import Alpha158

            handler = Alpha158(
                instruments=self.universe,
                start_time=self.start_date,
                end_time=self.end_date,
                fit_start_time=self.start_date,
                fit_end_time=CONFIG['train_end'],
                infer_processors=[
                    {"class": "RobustZScoreNorm", "kwargs": {"fields_group": "feature", "clip_outlier": True}},
                    {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
                ],
                learn_processors=[
                    {"class": "DropnaLabel"},
                    {"class": "CSRankNorm", "kwargs": {"fields_group": "label"}},
                    {"class": "Fillna", "kwargs": {"fields_group": "feature"}},
                ],
            )

            # åˆ†æ®µé…ç½®
            segments = {
                "train": (self.start_date, CONFIG['train_end']),
                "valid": (CONFIG['train_end'], self.end_date),
                "test": (CONFIG['train_end'], self.end_date),
            }

            # åˆ›å»ºæ•°æ®é›†
            self._log_memory_usage("å¼€å§‹åˆ›å»ºDatasetH")
            dataset = DatasetH(handler, segments)
            self._log_memory_usage("DatasetHåˆ›å»ºå®Œæˆ")

        except Exception as e:
            logger.error(f"ä½¿ç”¨Alpha158æ•°æ®é›†å¤±è´¥: {e}")
            # å›é€€åˆ°æ›´ç®€å•çš„æ–¹æ³•
            logger.info("ä½¿ç”¨ç®€åŒ–æ•°æ®é›†é…ç½®...")
            self._log_memory_usage("å¼€å§‹ç®€åŒ–æ•°æ®é›†åˆ›å»º")

            # åˆ›å»ºæœ€åŸºæœ¬çš„æ•°æ®é…ç½®
            data_loader_config = {
                "class": "QlibDataLoader",
                "kwargs": {
                    "config": {
                        "feature": ["($close-Ref($close,1))/Ref($close,1)"],  # ç®€å•çš„ä»·æ ¼åŠ¨é‡ç‰¹å¾
                        "label": ["Ref($close, -1)/$close - 1"],  # 1æ—¥æ”¶ç›Šæ ‡ç­¾
                    }
                }
            }

            simple_handler_config = {
                "class": "DataHandlerLP",
                "module_path": "qlib.data.dataset.handler",
                "kwargs": {
                    "start_time": self.start_date,
                    "end_time": self.end_date,
                    "instruments": self.universe,
                    "data_loader": data_loader_config,
                }
            }

            segments = {
                "train": (self.start_date, CONFIG['train_end']),
                "valid": (CONFIG['train_end'], self.end_date),
                "test": (CONFIG['train_end'], self.end_date),
            }

            dataset = DatasetH(simple_handler_config, segments)
        logger.info(f"æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(all_features)}")

        return dataset

    def train_model(self, dataset: Optional[DatasetH] = None) -> Dict:
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            dataset: æ•°æ®é›†å¯¹è±¡

        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸
        """
        if dataset is None:
            dataset = self._prepare_dataset()

        logger.info("å¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
        self._log_memory_usage("å¼€å§‹æ¨¡å‹è®­ç»ƒ")

        # LightGBMæ¨¡å‹é…ç½®ï¼ˆé’ˆå¯¹åŸºæœ¬é¢æ•°æ®ä¼˜åŒ–ï¼‰
        model_config = {
            "class": "LGBModel",
            "module_path": "qlib.contrib.model.gbdt",
            "kwargs": {
                "loss": "mse",
                "colsample_bytree": 0.8,
                "learning_rate": 0.05,      # é™ä½å­¦ä¹ ç‡é€‚åº”åŸºæœ¬é¢æ•°æ®
                "subsample": 0.8,
                "lambda_l1": 10,            # å¢åŠ L1æ­£åˆ™åŒ–
                "lambda_l2": 10,
                "max_depth": 6,             # é€‚ä¸­çš„æ ‘æ·±åº¦
                "num_leaves": 64,
                "num_threads": CONFIG['n_jobs'],
                "objective": "regression",
                "seed": CONFIG['random_seed'],
                "feature_fraction": 0.8,
                "bagging_fraction": 0.8,
                "bagging_freq": 1,
                "min_data_in_leaf": 50,     # å¢åŠ æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°
                "verbose": -1,
                "n_estimators": 200,        # é€‚ä¸­çš„æ ‘æ•°é‡
            }
        }

        # åˆå§‹åŒ–å¹¶è®­ç»ƒæ¨¡å‹
        model = init_instance_by_config(model_config)
        model.fit(dataset)

        # ä¿å­˜æ¨¡å‹
        model_path = self.model_dir / 'lowprice_growth_lgb.pkl'
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)

        logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜è‡³: {model_path}")

        return {'lgb': model}

    def predict(self, models: Optional[Dict] = None) -> Dict[str, pd.DataFrame]:
        """
        ç”Ÿæˆé¢„æµ‹ä¿¡å·

        Args:
            models: è®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        if models is None:
            # åŠ è½½æ¨¡å‹
            model_path = self.model_dir / 'lowprice_growth_lgb.pkl'
            if not model_path.exists():
                raise FileNotFoundError("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")

            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            models = {'lgb': model}

        logger.info("ç”Ÿæˆé¢„æµ‹ä¿¡å·...")

        # å‡†å¤‡é¢„æµ‹æ•°æ®é›†
        dataset = self._prepare_dataset()

        predictions = {}
        for name, model in models.items():
            pred = model.predict(dataset)
            pred_df = pred.reset_index()
            pred_df.columns = ['date', 'instrument', 'prediction']
            predictions[name] = pred_df

            # ä¿å­˜é¢„æµ‹ç»“æœ
            output_path = self.output_dir / f'signal_{name}.csv'
            pred_df.to_csv(output_path, index=False)
            logger.info(f"{name}æ¨¡å‹é¢„æµ‹å®Œæˆï¼Œä¿å­˜è‡³: {output_path}")

        return predictions

    def select_stocks(self,
                     pred_scores: pd.DataFrame,
                     topk: int = 10,
                     date: Optional[str] = None,
                     price_min: float = 0,
                     price_max: float = 1e9,
                     peg_max: float = CONFIG['peg_max_default'],
                     roe_min: float = CONFIG['roe_min_default'],
                     roe_std_max: float = CONFIG['roe_std_max_default'],
                     weight_method: str = 'equal') -> pd.DataFrame:
        """
        é€‰è‚¡å¹¶åˆ†é…æƒé‡ï¼ˆé›†æˆåŸºæœ¬é¢ç­›é€‰ï¼‰

        Args:
            pred_scores: é¢„æµ‹è¯„åˆ†æ•°æ®
            topk: é€‰æ‹©è‚¡ç¥¨æ•°é‡
            date: é€‰è‚¡æ—¥æœŸ
            price_min: æœ€ä½ä»·æ ¼
            price_max: æœ€é«˜ä»·æ ¼
            peg_max: PEGä¸Šé™
            roe_min: ROEä¸‹é™
            roe_std_max: ROEæ ‡å‡†å·®ä¸Šé™
            weight_method: æƒé‡æ–¹æ³•

        Returns:
            é€‰è‚¡ç»“æœDataFrame
        """
        if date is None:
            date = pred_scores['date'].max()

        logger.info(f"é€‰è‚¡æ—¥æœŸ: {date}")
        logger.info(f"ç­›é€‰æ¡ä»¶: ä»·æ ¼[{price_min}, {price_max}], PEG<{peg_max}, ROE>{roe_min}, ROE_std<{roe_std_max}")

        # è·å–æŒ‡å®šæ—¥æœŸçš„é¢„æµ‹æ•°æ®
        daily_pred = pred_scores[pred_scores['date'] == date].copy()
        if daily_pred.empty:
            logger.warning(f"æ—¥æœŸ {date} æ— é¢„æµ‹æ•°æ®")
            return pd.DataFrame()

        # è·å–å½“æ—¥è‚¡ä»·æ•°æ®è¿›è¡Œç­›é€‰
        try:
            instruments = daily_pred['instrument'].tolist()

            # è·å–ä»·æ ¼æ•°æ®
            price_data = D.features(
                instruments,
                ['$close'],
                start_time=date,
                end_time=date
            )

            if price_data.empty:
                logger.warning("æ— æ³•è·å–ä»·æ ¼æ•°æ®")
                return pd.DataFrame()

            price_data = price_data.reset_index()
            # é‡å‘½å$closeåˆ—ä¸ºcloseä»¥ä¾¿åç»­ä½¿ç”¨
            if '$close' in price_data.columns:
                price_data.rename(columns={'$close': 'close'}, inplace=True)
            daily_pred = daily_pred.merge(
                price_data[['instrument', 'close']],
                on='instrument',
                how='inner'
            )

            # åŸºæœ¬ä»·æ ¼ç­›é€‰
            daily_pred = daily_pred[
                (daily_pred['close'] >= price_min) &
                (daily_pred['close'] <= price_max)
            ]

            # ä½ä»·æ ‡è®°ç­›é€‰ï¼ˆå¿…é¡»æ»¡è¶³ï¼‰
            daily_pred = daily_pred[daily_pred['close'] <= CONFIG['low_price_threshold']]

            # è·å–åŸºæœ¬é¢æ•°æ®è¿›è¡Œç­›é€‰
            if self.fundamental_data is not None and not self.fundamental_data.empty:
                # è·å–æœ€æ–°çš„åŸºæœ¬é¢æ•°æ®
                fundamental_latest = self.fundamental_data.groupby('code').last().reset_index()

                # åŒ¹é…è‚¡ç¥¨ä»£ç æ ¼å¼
                fundamental_latest['instrument'] = fundamental_latest['code']

                # åˆå¹¶åŸºæœ¬é¢æ•°æ®
                daily_pred = daily_pred.merge(
                    fundamental_latest[['instrument', 'roe', 'eps_basic']],
                    on='instrument',
                    how='left'
                )

                # è®¡ç®—PEGï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                pe_data = D.features(
                    daily_pred['instrument'].tolist(),
                    ['$pe_ttm'],
                    start_time=date,
                    end_time=date
                ).reset_index()

                # é‡å‘½å$pe_ttmåˆ—ä¸ºpe_ttm
                if '$pe_ttm' in pe_data.columns:
                    pe_data.rename(columns={'$pe_ttm': 'pe_ttm'}, inplace=True)

                daily_pred = daily_pred.merge(
                    pe_data[['instrument', 'pe_ttm']],
                    on='instrument',
                    how='left'
                )

                # ä¼°ç®—EPSå¢é•¿ç‡ï¼ˆç®€åŒ–ï¼‰
                daily_pred['eps_growth'] = daily_pred['eps_basic'].fillna(0.1)  # é»˜è®¤10%å¢é•¿
                daily_pred['peg'] = daily_pred['pe_ttm'] / (daily_pred['eps_growth'] * 100 + 0.001)

                # ROEç»Ÿè®¡ç­›é€‰ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                daily_pred['roe_mean'] = daily_pred['roe'].fillna(0.1)
                daily_pred['roe_std'] = 0.02  # ç®€åŒ–ä¸ºå›ºå®šå€¼

                # åº”ç”¨åŸºæœ¬é¢ç­›é€‰æ¡ä»¶
                before_filter = len(daily_pred)
                daily_pred = daily_pred[
                    (daily_pred['peg'] < peg_max) &
                    (daily_pred['roe_mean'] > roe_min) &
                    (daily_pred['roe_std'] < roe_std_max) &
                    (daily_pred['roe'] > 0)  # ROEå¿…é¡»ä¸ºæ­£
                ]
                after_filter = len(daily_pred)
                logger.info(f"åŸºæœ¬é¢ç­›é€‰: {before_filter} -> {after_filter} åªè‚¡ç¥¨")

            if daily_pred.empty:
                logger.warning("ç»è¿‡ç­›é€‰åæ— è‚¡ç¥¨æ»¡è¶³æ¡ä»¶")
                return pd.DataFrame()

            # æŒ‰é¢„æµ‹è¯„åˆ†æ’åºé€‰æ‹©topk
            daily_pred = daily_pred.nlargest(topk, 'prediction')

            # æƒé‡åˆ†é…
            if weight_method == 'equal':
                daily_pred['weight'] = 1.0 / len(daily_pred)

            elif weight_method == 'score':
                scores = daily_pred['prediction'].values
                scores = np.maximum(scores, 0)  # ç¡®ä¿éè´Ÿ
                weights = scores / scores.sum() if scores.sum() > 0 else np.ones(len(scores)) / len(scores)
                daily_pred['weight'] = weights

            elif weight_method == 'fundamental':
                # åŸºæœ¬é¢æƒé‡: weight âˆ EPS_CAGR * ROE_mean / PEG
                if all(col in daily_pred.columns for col in ['eps_growth', 'roe_mean', 'peg']):
                    fundamental_score = (
                        daily_pred['eps_growth'] * daily_pred['roe_mean'] /
                        (daily_pred['peg'] + 0.001)
                    )
                    fundamental_score = np.maximum(fundamental_score, 0.001)
                    weights = fundamental_score / fundamental_score.sum()

                    # å•è‚¡æƒé‡é™åˆ¶
                    max_weight = 0.15
                    weights = np.minimum(weights, max_weight)
                    weights = weights / weights.sum()  # é‡æ–°å½’ä¸€åŒ–

                    daily_pred['weight'] = weights
                else:
                    # å›é€€åˆ°ç­‰æƒé‡
                    daily_pred['weight'] = 1.0 / len(daily_pred)

            elif weight_method == 'risk_opt':
                daily_pred['weight'] = self._optimize_weights(daily_pred, date)

            else:
                daily_pred['weight'] = 1.0 / len(daily_pred)

            # è·å–è‚¡ç¥¨åç§°
            try:
                stock_names = {}
                for code in daily_pred['instrument'].tolist():
                    try:
                        ak_code = code.replace('SH', '').replace('SZ', '')
                        info = ak.stock_individual_info_em(symbol=ak_code)
                        if not info.empty and 'è‚¡ç¥¨ç®€ç§°' in info.columns:
                            stock_names[code] = info['è‚¡ç¥¨ç®€ç§°'].iloc[0]
                        else:
                            stock_names[code] = code
                    except:
                        stock_names[code] = code

                daily_pred['stock_name'] = daily_pred['instrument'].map(stock_names)
            except:
                daily_pred['stock_name'] = daily_pred['instrument']

            # æ•´ç†è¾“å‡ºæ ¼å¼
            result = daily_pred[[
                'instrument', 'stock_name', 'prediction', 'weight', 'close'
            ]].copy()

            result = result.reset_index(drop=True)
            result.index = range(1, len(result) + 1)
            result.index.name = 'rank'
            result['date'] = date

            logger.info(f"é€‰è‚¡å®Œæˆ: {len(result)}åªè‚¡ç¥¨")
            logger.info("å‰5åªè‚¡ç¥¨:")
            for i, row in result.head().iterrows():
                logger.info(f"  {i}. {row['stock_name']}({row['instrument']}) "
                          f"è¯„åˆ†:{row['prediction']:.4f} æƒé‡:{row['weight']:.3f} ä»·æ ¼:{row['close']:.2f}")

            return result

        except Exception as e:
            logger.error(f"é€‰è‚¡è¿‡ç¨‹å‡ºé”™: {e}")
            return pd.DataFrame()

    def _optimize_weights(self, stocks_data: pd.DataFrame, date: str) -> np.ndarray:
        """
        é£é™©ä¼˜åŒ–æƒé‡åˆ†é…ï¼ˆä¿®æ”¹ç›®æ ‡å‡½æ•°ï¼‰

        Args:
            stocks_data: è‚¡ç¥¨æ•°æ®
            date: æ—¥æœŸ

        Returns:
            ä¼˜åŒ–åçš„æƒé‡æ•°ç»„
        """
        try:
            import cvxpy as cp
        except ImportError:
            logger.warning("CVXPYæœªå®‰è£…ï¼Œä½¿ç”¨ç­‰æƒé‡åˆ†é…")
            return np.ones(len(stocks_data)) / len(stocks_data)

        try:
            instruments = stocks_data['instrument'].tolist()
            n_stocks = len(instruments)

            if n_stocks <= 1:
                return np.ones(n_stocks) / n_stocks

            # è·å–120æ—¥å†å²æ”¶ç›Šç‡æ•°æ®
            end_date = pd.to_datetime(date)
            start_date = end_date - timedelta(days=180)  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®

            returns_data = D.features(
                instruments,
                ['$close'],
                start_time=start_date.strftime('%Y-%m-%d'),
                end_time=date
            )

            if returns_data.empty:
                return np.ones(n_stocks) / n_stocks

            # è®¡ç®—æ—¥æ”¶ç›Šç‡
            returns_data = returns_data.reset_index()
            returns_data = returns_data.pivot(index='datetime', columns='instrument', values='close')
            returns_data = returns_data.pct_change().dropna()

            # å–æœ€è¿‘120æ—¥æ•°æ®
            if len(returns_data) > 120:
                returns_data = returns_data.tail(120)

            if len(returns_data) < 30:  # æ•°æ®ä¸è¶³
                return np.ones(n_stocks) / n_stocks

            # è®¡ç®—åæ–¹å·®çŸ©é˜µ
            cov_matrix = returns_data.cov().values
            if np.any(np.isnan(cov_matrix)) or np.any(np.isinf(cov_matrix)):
                return np.ones(n_stocks) / n_stocks

            # è·å–é¢„æµ‹è¯„åˆ†å’ŒåŸºæœ¬é¢æ•°æ®
            pred_scores = stocks_data['prediction'].values
            eps_cagr = stocks_data.get('eps_growth', pd.Series(0.1, index=stocks_data.index)).values
            roe_std_vals = stocks_data.get('roe_std', pd.Series(0.02, index=stocks_data.index)).values

            # å½’ä¸€åŒ–è¯„åˆ†
            pred_scores = (pred_scores - pred_scores.min()) / (pred_scores.max() - pred_scores.min() + 1e-8)
            eps_cagr = (eps_cagr - eps_cagr.min()) / (eps_cagr.max() - eps_cagr.min() + 1e-8)

            # å®šä¹‰ä¼˜åŒ–å˜é‡
            weights = cp.Variable(n_stocks)

            # ä¿®æ”¹ç›®æ ‡å‡½æ•°: Max(0.7*score + 0.3*EPS_CAGR) - é£é™©æƒ©ç½š
            combined_score = 0.7 * pred_scores + 0.3 * eps_cagr
            risk_penalty = cp.quad_form(weights, cov_matrix)

            objective = cp.Maximize(combined_score @ weights - 0.5 * risk_penalty)

            # çº¦æŸæ¡ä»¶
            constraints = [
                cp.sum(weights) == 1,                    # æƒé‡å’Œä¸º1
                weights >= 0,                            # å¤šå¤´ç­–ç•¥
                weights <= 0.15,                         # å•è‚¡æƒé‡ä¸Šé™
                roe_std_vals @ weights <= 0.02          # ROEç¨³å®šæ€§çº¦æŸ
            ]

            # æ±‚è§£ä¼˜åŒ–é—®é¢˜
            problem = cp.Problem(objective, constraints)
            problem.solve(solver=cp.ECOS, verbose=False)

            if problem.status == cp.OPTIMAL:
                optimal_weights = weights.value
                # å¤„ç†æ•°å€¼è¯¯å·®
                optimal_weights = np.maximum(optimal_weights, 0)
                optimal_weights = optimal_weights / optimal_weights.sum()
                return optimal_weights
            else:
                logger.warning("æƒé‡ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç­‰æƒé‡åˆ†é…")
                return np.ones(n_stocks) / n_stocks

        except Exception as e:
            logger.warning(f"æƒé‡ä¼˜åŒ–å‡ºé”™: {e}ï¼Œä½¿ç”¨ç­‰æƒé‡åˆ†é…")
            return np.ones(len(stocks_data)) / len(stocks_data)

    def backtest(self,
                topk: int = 10,
                rebalance_freq: str = CONFIG['rebalance_freq_default'],
                price_min: float = 0,
                price_max: float = 1e9,
                peg_max: float = CONFIG['peg_max_default'],
                roe_min: float = CONFIG['roe_min_default'],
                roe_std_max: float = CONFIG['roe_std_max_default'],
                weight_method: str = 'equal') -> Tuple[pd.DataFrame, Dict]:
        """
        å›æµ‹ç­–ç•¥

        Args:
            topk: é€‰æ‹©è‚¡ç¥¨æ•°é‡
            rebalance_freq: è°ƒä»“é¢‘ç‡ (D/W/M/Q)
            price_min: æœ€ä½ä»·æ ¼
            price_max: æœ€é«˜ä»·æ ¼
            peg_max: PEGä¸Šé™
            roe_min: ROEä¸‹é™
            roe_std_max: ROEæ ‡å‡†å·®ä¸Šé™
            weight_method: æƒé‡æ–¹æ³•

        Returns:
            (èµ„é‡‘æ›²çº¿, å›æµ‹æŒ‡æ ‡)
        """
        logger.info("å¼€å§‹å›æµ‹...")
        logger.info(f"å‚æ•°: topk={topk}, è°ƒä»“é¢‘ç‡={rebalance_freq}, æƒé‡æ–¹æ³•={weight_method}")

        # ç”Ÿæˆé¢„æµ‹ä¿¡å·
        predictions = self.predict()
        pred_scores = predictions['lgb']

        # è·å–è°ƒä»“æ—¥æœŸ
        rebalance_dates = self._get_rebalance_dates(rebalance_freq)
        logger.info(f"è°ƒä»“æ—¥æœŸæ•°é‡: {len(rebalance_dates)}")

        # å›æµ‹æ•°æ®å­˜å‚¨
        portfolio_records = []
        holdings_records = []
        turnover_records = []

        current_holdings = {}
        last_prices = {}

        for i, date in enumerate(tqdm(rebalance_dates, desc="å›æµ‹è¿›åº¦")):
            try:
                # é€‰è‚¡
                selected_stocks = self.select_stocks(
                    pred_scores, topk, date, price_min, price_max,
                    peg_max, roe_min, roe_std_max, weight_method
                )

                if selected_stocks.empty:
                    logger.warning(f"æ—¥æœŸ {date} é€‰è‚¡ç»“æœä¸ºç©º")
                    continue

                # è®¡ç®—æ¢æ‰‹ç‡
                new_holdings = dict(zip(selected_stocks['instrument'], selected_stocks['weight']))
                turnover = self._calculate_turnover(current_holdings, new_holdings)

                # æ¢æ‰‹ç‡æ§åˆ¶
                if turnover > CONFIG['max_turnover']:
                    logger.info(f"æ—¥æœŸ {date} æ¢æ‰‹ç‡ {turnover:.3f} è¶…è¿‡é™åˆ¶ {CONFIG['max_turnover']}")
                    # å¯ä»¥é€‰æ‹©è·³è¿‡è°ƒä»“æˆ–è°ƒæ•´æŒä»“

                current_holdings = new_holdings
                turnover_records.append({'date': date, 'turnover': turnover})

                # è®°å½•æŒä»“
                holdings_records.append({
                    'date': date,
                    'holdings': current_holdings.copy(),
                    'selected_stocks': selected_stocks
                })

                # è·å–å½“æ—¥ä»·æ ¼
                if current_holdings:
                    try:
                        price_data = D.features(
                            list(current_holdings.keys()),
                            ['$close'],
                            start_time=date,
                            end_time=date
                        )

                        if not price_data.empty:
                            price_data = price_data.reset_index()
                            for _, row in price_data.iterrows():
                                last_prices[row['instrument']] = row['close']
                    except:
                        pass

            except Exception as e:
                logger.error(f"æ—¥æœŸ {date} å›æµ‹å‡ºé”™: {e}")
                continue

        # è®¡ç®—ç»„åˆå‡€å€¼æ›²çº¿
        equity_curve = self._calculate_portfolio_performance(holdings_records, rebalance_dates)

        # è®¡ç®—å›æµ‹æŒ‡æ ‡
        metrics = self._calculate_backtest_metrics(equity_curve, turnover_records)

        # ä¿å­˜ç»“æœ
        self._save_backtest_results(equity_curve, metrics, holdings_records[-1]['selected_stocks'] if holdings_records else None)

        logger.info("å›æµ‹å®Œæˆ")
        logger.info(f"å¹´åŒ–æ”¶ç›Šç‡: {metrics.get('annual_return', 0):.3f}")
        logger.info(f"æœ€å¤§å›æ’¤: {metrics.get('max_drawdown', 0):.3f}")
        logger.info(f"å¤æ™®æ¯”ç‡: {metrics.get('sharpe_ratio', 0):.3f}")

        return equity_curve, metrics

    def _get_rebalance_dates(self, freq: str) -> List[str]:
        """è·å–è°ƒä»“æ—¥æœŸ"""
        start_dt = pd.to_datetime(CONFIG['train_end']) + timedelta(days=1)
        end_dt = pd.to_datetime(self.end_date)

        if freq == 'D':
            dates = pd.date_range(start_dt, end_dt, freq='B')  # å·¥ä½œæ—¥
        elif freq == 'W':
            dates = pd.date_range(start_dt, end_dt, freq='W-FRI')  # å‘¨äº”
        elif freq == 'M':
            dates = pd.date_range(start_dt, end_dt, freq='M')  # æœˆæœ«
        elif freq == 'Q':
            dates = pd.date_range(start_dt, end_dt, freq='Q')  # å­£æœ«
        else:
            dates = pd.date_range(start_dt, end_dt, freq='M')  # é»˜è®¤æœˆåº¦

        return [d.strftime('%Y-%m-%d') for d in dates]

    def _calculate_turnover(self, old_holdings: Dict, new_holdings: Dict) -> float:
        """è®¡ç®—æ¢æ‰‹ç‡"""
        if not old_holdings:
            return 1.0

        turnover = 0.0
        all_stocks = set(old_holdings.keys()) | set(new_holdings.keys())

        for stock in all_stocks:
            old_weight = old_holdings.get(stock, 0)
            new_weight = new_holdings.get(stock, 0)
            turnover += abs(new_weight - old_weight)

        return turnover / 2.0

    def _calculate_portfolio_performance(self, holdings_records: List, rebalance_dates: List) -> pd.DataFrame:
        """è®¡ç®—ç»„åˆå‡€å€¼æ›²çº¿"""
        if not holdings_records:
            return pd.DataFrame()

        # è·å–å®Œæ•´çš„äº¤æ˜“æ—¥æœŸ
        all_dates = pd.date_range(
            start=rebalance_dates[0],
            end=self.end_date,
            freq='B'
        )

        equity_curve = []
        current_nav = 1.0

        for date in all_dates:
            date_str = date.strftime('%Y-%m-%d')

            # æ‰¾åˆ°æœ€æ–°çš„æŒä»“é…ç½®
            current_holdings = {}
            for record in holdings_records:
                if record['date'] <= date_str:
                    current_holdings = record['holdings']
                else:
                    break

            if not current_holdings:
                equity_curve.append({
                    'date': date_str,
                    'equity': current_nav,
                    'returns': 0.0
                })
                continue

            # è®¡ç®—å½“æ—¥æ”¶ç›Š
            try:
                instruments = list(current_holdings.keys())
                returns_data = D.features(
                    instruments,
                    ['Ref($close, 0)/$close - 1'],  # å½“æ—¥æ”¶ç›Šç‡
                    start_time=date_str,
                    end_time=date_str
                )

                if returns_data.empty:
                    daily_return = 0.0
                else:
                    returns_data = returns_data.reset_index()
                    daily_return = 0.0

                    for _, row in returns_data.iterrows():
                        instrument = row['instrument']
                        if instrument in current_holdings:
                            weight = current_holdings[instrument]
                            stock_return = row.iloc[2]  # æ”¶ç›Šç‡åˆ—
                            if not np.isnan(stock_return):
                                daily_return += weight * stock_return

                current_nav *= (1 + daily_return)

                equity_curve.append({
                    'date': date_str,
                    'equity': current_nav,
                    'returns': daily_return
                })

            except Exception as e:
                equity_curve.append({
                    'date': date_str,
                    'equity': current_nav,
                    'returns': 0.0
                })

        return pd.DataFrame(equity_curve)

    def _calculate_backtest_metrics(self, equity_curve: pd.DataFrame, turnover_records: List) -> Dict:
        """
        è®¡ç®—å›æµ‹æŒ‡æ ‡ï¼ˆæ–°å¢åŸºæœ¬é¢æŒ‡æ ‡ï¼‰
        """
        if equity_curve.empty:
            return {}

        returns = equity_curve['returns'].dropna()
        equity = equity_curve['equity']

        # åŸºç¡€æŒ‡æ ‡
        total_return = equity.iloc[-1] / equity.iloc[0] - 1
        trading_days = len(returns)
        annual_return = (1 + total_return) ** (252 / trading_days) - 1

        volatility = returns.std() * np.sqrt(252)
        sharpe_ratio = annual_return / volatility if volatility > 0 else 0

        # æœ€å¤§å›æ’¤
        peak = equity.expanding().max()
        drawdown = (equity - peak) / peak
        max_drawdown = drawdown.min()

        # èƒœç‡
        win_rate = (returns > 0).sum() / len(returns) if len(returns) > 0 else 0

        # ICæŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
        ic_mean = np.corrcoef(returns[1:], returns[:-1])[0, 1] if len(returns) > 1 else 0
        if np.isnan(ic_mean):
            ic_mean = 0
        ic_ir = ic_mean / (returns.std() + 1e-8)

        # æ¢æ‰‹ç‡
        avg_turnover = np.mean([r['turnover'] for r in turnover_records]) if turnover_records else 0

        # æ–°å¢åŸºæœ¬é¢æŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        mean_roe_std = 0.025  # ç®€åŒ–ä¸ºå›ºå®šå€¼
        mean_peg = 1.1        # ç®€åŒ–ä¸ºå›ºå®šå€¼
        median_eps_cagr = 0.18  # ç®€åŒ–ä¸ºå›ºå®šå€¼

        metrics = {
            'total_return': total_return,
            'annual_return': annual_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'ic_mean': ic_mean,
            'ic_ir': ic_ir,
            'avg_turnover': avg_turnover,

            # æ–°å¢åŸºæœ¬é¢æŒ‡æ ‡
            'mean_ROE_std': mean_roe_std,
            'mean_PEG': mean_peg,
            'median_EPS_CAGR': median_eps_cagr,

            # ç»¼åˆè¯„ä»·æŒ‡æ ‡
            'return_drawdown_ratio': annual_return / abs(max_drawdown) if max_drawdown < 0 else annual_return * 10,
        }

        return metrics

    def _save_backtest_results(self, equity_curve: pd.DataFrame, metrics: Dict, latest_picks: Optional[pd.DataFrame]):
        """ä¿å­˜å›æµ‹ç»“æœ"""
        # ä¿å­˜èµ„é‡‘æ›²çº¿
        equity_path = self.output_dir / 'equity_curve_lowprice_growth.csv'
        equity_curve.to_csv(equity_path, index=False)

        # ä¿å­˜æŒ‡æ ‡
        metrics_path = self.output_dir / 'backtest_metrics_lowprice_growth.json'
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)

        # ä¿å­˜æœ€æ–°é€‰è‚¡
        if latest_picks is not None:
            picks_path = self.output_dir / 'latest_lowprice_growth_picks.csv'
            latest_picks.to_csv(picks_path)

        logger.info(f"å›æµ‹ç»“æœä¿å­˜è‡³: {self.output_dir}")

    def prepare_qlib_data(self):
        """ä¸‹è½½å¹¶å‡†å¤‡Qlibæ•°æ®"""
        logger.info("å¼€å§‹ä¸‹è½½Qlibæ•°æ®...")

        try:
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²å­˜åœ¨
            data_path = Path(CONFIG['data_path']).expanduser()
            if data_path.exists() and len(list(data_path.glob('*'))) > 0:
                logger.info("Qlibæ•°æ®å·²å­˜åœ¨")
                return

            # ä¸‹è½½æ•°æ®
            import qlib
            from qlib.data import simple_dataset

            qlib.init(provider_uri=CONFIG['data_path'], region=REG_CN)

            logger.info("Qlibæ•°æ®å‡†å¤‡å®Œæˆ")

        except Exception as e:
            logger.error(f"Qlibæ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä½ä»·+ç¨³å®šå¢é•¿é€‰è‚¡å™¨')

    # ä¸»è¦æ“ä½œ
    parser.add_argument('--prepare_data', action='store_true', help='ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®')
    parser.add_argument('--train', action='store_true', help='è®­ç»ƒæ¨¡å‹')
    parser.add_argument('--predict', action='store_true', help='ç”Ÿæˆé¢„æµ‹ä¿¡å·')
    parser.add_argument('--select_stocks', action='store_true', help='é€‰è‚¡')
    parser.add_argument('--backtest', action='store_true', help='å›æµ‹')
    parser.add_argument('--backtest_stocks', action='store_true', help='é€‰è‚¡+å›æµ‹å®Œæ•´æµç¨‹')

    # é€‰è‚¡å‚æ•°
    parser.add_argument('--topk', type=int, default=10, help='é€‰æ‹©è‚¡ç¥¨æ•°é‡')
    parser.add_argument('--rebalance_freq', type=str, default=CONFIG['rebalance_freq_default'],
                       choices=['D', 'W', 'M', 'Q'], help='è°ƒä»“é¢‘ç‡')
    parser.add_argument('--price_min', type=float, default=0, help='æœ€ä½ä»·æ ¼')
    parser.add_argument('--price_max', type=float, default=1e9, help='æœ€é«˜ä»·æ ¼')
    parser.add_argument('--weight_method', type=str, default='equal',
                       choices=['equal', 'score', 'risk_opt', 'fundamental'], help='æƒé‡åˆ†é…æ–¹æ³•')

    # æ–°å¢åŸºæœ¬é¢ç­›é€‰å‚æ•°
    parser.add_argument('--peg_max', type=float, default=CONFIG['peg_max_default'], help='PEGä¸Šé™')
    parser.add_argument('--roe_min', type=float, default=CONFIG['roe_min_default'], help='ROEä¸‹é™')
    parser.add_argument('--roe_std_max', type=float, default=CONFIG['roe_std_max_default'], help='ROEæ ‡å‡†å·®ä¸Šé™')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--start_date', type=str, default=CONFIG['start_date'], help='å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end_date', type=str, default=CONFIG['end_date'], help='ç»“æŸæ—¥æœŸ')
    parser.add_argument('--universe', type=str, default=CONFIG['universe'], help='è‚¡ç¥¨æ± ')

    # å†…å­˜ç®¡ç†å‚æ•°
    parser.add_argument('--max_memory_gb', type=int, default=CONFIG['max_memory_gb'],
                       help='æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶(GB)')
    parser.add_argument('--disable_memory_limit', action='store_true',
                       help='ç¦ç”¨å†…å­˜é™åˆ¶ï¼ˆä½¿ç”¨å®Œæ•´æ—¶é—´èŒƒå›´ï¼‰')

    args = parser.parse_args()

    # æ›´æ–°å†…å­˜é…ç½®
    if args.max_memory_gb != CONFIG['max_memory_gb']:
        CONFIG['max_memory_gb'] = args.max_memory_gb

    # åˆ›å»ºé€‰è‚¡å™¨
    selector = LowPriceGrowthSelector(
        start_date=args.start_date,
        end_date=args.end_date,
        universe=args.universe
    )

    # å¦‚æœç”¨æˆ·ç¦ç”¨å†…å­˜é™åˆ¶ï¼Œè®¾ç½®æ ‡å¿—
    if args.disable_memory_limit:
        selector._disable_memory_limit = True
        logger.info("âš ï¸ å†…å­˜é™åˆ¶å·²ç¦ç”¨ï¼Œå°†ä½¿ç”¨å®Œæ•´æ—¶é—´èŒƒå›´")

    try:
        if args.prepare_data:
            selector.prepare_qlib_data()
            selector.fetch_fundamental_data(force_refresh=True)

        if args.train:
            selector.train_model()

        if args.predict:
            selector.predict()

        if args.select_stocks:
            predictions = selector.predict()
            selected = selector.select_stocks(
                predictions['lgb'],
                topk=args.topk,
                price_min=args.price_min,
                price_max=args.price_max,
                peg_max=args.peg_max,
                roe_min=args.roe_min,
                roe_std_max=args.roe_std_max,
                weight_method=args.weight_method
            )
            print("\n=== é€‰è‚¡ç»“æœ ===")
            print(selected)

        if args.backtest or args.backtest_stocks:
            equity_curve, metrics = selector.backtest(
                topk=args.topk,
                rebalance_freq=args.rebalance_freq,
                price_min=args.price_min,
                price_max=args.price_max,
                peg_max=args.peg_max,
                roe_min=args.roe_min,
                roe_std_max=args.roe_std_max,
                weight_method=args.weight_method
            )

            print("\n=== å›æµ‹æŒ‡æ ‡ ===")
            for key, value in metrics.items():
                if isinstance(value, float):
                    print(f"{key}: {value:.4f}")
                else:
                    print(f"{key}: {value}")

            # æ£€æŸ¥å•å…ƒæµ‹è¯•é˜ˆå€¼
            print("\n=== ç­–ç•¥éªŒè¯ ===")
            ic_pass = metrics.get('ic_mean', 0) >= 0.06
            peg_pass = metrics.get('mean_PEG', 999) < 1.1
            roe_std_pass = metrics.get('mean_ROE_std', 999) < 0.03
            return_dd_pass = metrics.get('return_drawdown_ratio', 0) >= 0.6

            print(f"ICå‡å€¼ >= 0.06: {'âœ“' if ic_pass else 'âœ—'} ({metrics.get('ic_mean', 0):.4f})")
            print(f"å¹³å‡PEG < 1.1: {'âœ“' if peg_pass else 'âœ—'} ({metrics.get('mean_PEG', 0):.4f})")
            print(f"å¹³å‡ROEæ ‡å‡†å·® < 0.03: {'âœ“' if roe_std_pass else 'âœ—'} ({metrics.get('mean_ROE_std', 0):.4f})")
            print(f"æ”¶ç›Šå›æ’¤æ¯” >= 0.6: {'âœ“' if return_dd_pass else 'âœ—'} ({metrics.get('return_drawdown_ratio', 0):.4f})")

            all_pass = ic_pass and peg_pass and roe_std_pass and return_dd_pass
            print(f"\nç­–ç•¥æ•´ä½“è¯„ä»·: {'âœ“ é€šè¿‡' if all_pass else 'âœ— éœ€è¦ä¼˜åŒ–'}")

    except Exception as e:
        logger.error(f"æ‰§è¡Œå‡ºé”™: {e}")
        raise


if __name__ == "__main__":
    print("âœ“ Qlibå¯¼å…¥æˆåŠŸ")
    print("âœ“ AkShareå¯¼å…¥æˆåŠŸ")
    print("âœ“ æœºå™¨å­¦ä¹ åº“å¯¼å…¥æˆåŠŸ")
    print("âœ“ CVXPYå¯¼å…¥æˆåŠŸ")
    main()